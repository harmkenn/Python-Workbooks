{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver (choose your preferred browser)\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "\n",
    "\n",
    "# Add other desired options here\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get('https://www.bls.gov/ooh/home.htm')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver (choose your preferred browser)\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "options = webdriver.ChromeOptions()\n",
    "# Add other desired options here\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Open the webpage\n",
    "driver.get('https://www.bls.gov/ooh/home.htm')\n",
    "\n",
    "# Wait for some time to ensure the page is fully loaded\n",
    "time.sleep(5)\n",
    "\n",
    "# Get the page source\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Parse the page source using BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find all <a> tags with href containing '/ooh/'\n",
    "links = soup.find_all('a', href=lambda href: href and '/ooh/' in href)\n",
    "\n",
    "# Extract the href and text of each link\n",
    "link_data = []\n",
    "for link in links:\n",
    "    link_data.append({'href': link['href'], 'text': link.text.strip()})\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(link_data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_links = pd.read_csv('links.csv')\n",
    "pd_links = pd_links[~pd_links['text'].str.contains('view', na=False)]\n",
    "# Drop rows with index from 0 to 9\n",
    "pd_links.drop(index=pd_links.index[:9], inplace=True)\n",
    "\n",
    "# Drop rows with index from 700 to 750\n",
    "pd_links.drop(index=pd_links.index[358:], inplace=True)\n",
    "# Add a new column named 'type' and set initial value as 'occupation'\n",
    "pd_links['type'] = 'occupation'\n",
    "\n",
    "# Mark the first 23 rows as 'group' in the 'type' column\n",
    "pd_links.loc[:33, 'type'] = 'group'\n",
    "pd_links['grp'] = pd_links['href'].str.split('/').str[2]\n",
    "\n",
    "vl = pd_links.iloc[:25, [1, 3]].copy()\n",
    "\n",
    "pd_links = pd_links.merge(vl, on='grp', how='left')\n",
    "\n",
    "pd_links.rename(columns={'text_y': 'group'}, inplace=True)\n",
    "pd_links.rename(columns={'text_x': 'Occupation'}, inplace=True)\n",
    "\n",
    "pd_links_sorted = pd_links.sort_values(by=['group', 'type', 'Occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "pd_links.to_csv('BLSdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "BLSdata = pd.read_csv('BLSdata.csv')\n",
    "OCCdata =  BLSdata[BLSdata['type']=='occupation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "for url in OCCdata['href']:\n",
    "    page = f'https://www.bls.gov{url}'\n",
    "    response = requests.get(page)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty lists to store extracted data\n",
    "median_pay = []\n",
    "hourly_pay = []\n",
    "\n",
    "for url in OCCdata['href']:\n",
    "    page = f'https://www.bls.gov{url}'\n",
    "    response = requests.get(page)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <td> elements\n",
    "    td_elements = soup.find_all('td')\n",
    "    found = False\n",
    "    \n",
    "    # Iterate through all <td> elements to find median pay\n",
    "    for td_element in td_elements:\n",
    "        if \"Median Pay\" in td_element.text:\n",
    "            found = True\n",
    "            data = td_element.text.strip()\n",
    "            median_pay.append(data)\n",
    "\n",
    "            # Extract hourly pay if available\n",
    "            br_element = td_element.find_next('br')\n",
    "            if br_element:\n",
    "                hourly_pay_text = br_element.next_sibling.strip()\n",
    "                hourly_pay.append(hourly_pay_text)\n",
    "            else:\n",
    "                hourly_pay.append(None)\n",
    "            break\n",
    "    \n",
    "    # If median pay is not found, append None for both median_pay and hourly_pay\n",
    "    if not found:\n",
    "        median_pay.append(None)\n",
    "        hourly_pay.append(None)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    'Median Pay': median_pay,\n",
    "    'Hourly Pay': hourly_pay\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Now you have a DataFrame containing the extracted data\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   URL Median Pay Education  \\\n",
      "0    /ooh/personal-care-and-service/funeral-service...        NaN       NaN   \n",
      "1    /ooh/business-and-financial/insurance-underwri...        NaN       NaN   \n",
      "2    /ooh/computer-and-information-technology/compu...        NaN       NaN   \n",
      "3       /ooh/math/mathematicians-and-statisticians.htm        NaN       NaN   \n",
      "4    /ooh/life-physical-and-social-science/atmosphe...        NaN       NaN   \n",
      "..                                                 ...        ...       ...   \n",
      "328  /ooh/transportation-and-material-moving/taxi-d...        NaN       NaN   \n",
      "329  /ooh/community-and-social-service/community-he...        NaN       NaN   \n",
      "330           /ooh/healthcare/medical-dosimetrists.htm        NaN       NaN   \n",
      "331  /ooh/installation-maintenance-and-repair/calib...        NaN       NaN   \n",
      "332  /ooh/management/entertainment-and-recreation-m...        NaN       NaN   \n",
      "\n",
      "    Experience  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "3          NaN  \n",
      "4          NaN  \n",
      "..         ...  \n",
      "328        NaN  \n",
      "329        NaN  \n",
      "330        NaN  \n",
      "331        NaN  \n",
      "332        NaN  \n",
      "\n",
      "[333 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame to store extracted data (modify column names as needed)\n",
    "job_data_df = pd.DataFrame(columns=['URL', 'Median Pay', 'Education', 'Experience'])\n",
    "\n",
    "for url in OCCdata['href']:\n",
    "  page = f'https://www.bls.gov{url}'\n",
    "  response = requests.get(page)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "  # Extract data\n",
    "  job_data = {}\n",
    "  job_data['URL'] = url  # Store URL for reference\n",
    "\n",
    "  # Target table row for Median Pay\n",
    "  table_row = soup.find('tr', {'class': 'quickfacts'})  # Look for rows with class 'quickfacts'\n",
    "  if table_row:\n",
    "    data_cell = table_row.find('th', text=lambda text: text and text.startswith('2022 Median Pay'))  # Find header containing '2022 Median Pay'\n",
    "    if data_cell:\n",
    "      # Get the text content within the table cell, excluding child elements (anchor tag)\n",
    "      job_data['Median Pay'] = data_cell.text.strip().split('\\n')[0]  # Get first line, remove whitespace\n",
    "    else:\n",
    "      job_data['Median Pay'] = \"\"  # Assign empty string if header not found\n",
    "\n",
    "  # Target table row for Education (similar approach)\n",
    "  table_row = soup.find('tr', {'class': 'quickfacts'})\n",
    "  if table_row:\n",
    "    data_cell = table_row.find('th', text=lambda text: text and text.startswith('Typical Entry-Level Education'))\n",
    "    if data_cell:\n",
    "      job_data['Education'] = data_cell.text.strip().split('\\n')[0]  # Get first line, remove whitespace\n",
    "    else:\n",
    "      job_data['Education'] = \"\"\n",
    "\n",
    "  # Target table row for Experience (similar approach)\n",
    "  table_row = soup.find('tr', {'class': 'quickfacts'})\n",
    "  if table_row:\n",
    "    data_cell = table_row.find('th', text=lambda text: text and text.startswith('Work Experience'))\n",
    "    if data_cell:\n",
    "      job_data['Experience'] = data_cell.text.strip().split('\\n')[0]  # Get first line, remove whitespace\n",
    "    else:\n",
    "      job_data['Experience'] = \"\"\n",
    "\n",
    "  # ... Extract and store other desired data points ...\n",
    "\n",
    "  # Append data to DataFrame\n",
    "  job_data_df = pd.concat([job_data_df, pd.DataFrame([job_data])], ignore_index=True)\n",
    "\n",
    "print(job_data_df)  # Print the DataFrame containing extracted data (may include NaN for missing data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
