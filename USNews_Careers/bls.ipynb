{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Data saved to 'scraped_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape layer two (sub-occupations)\n",
    "def scrape_layer_two(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    sub_occupations = []\n",
    "    \n",
    "    # Find the table containing sub-occupations\n",
    "    table = soup.find('table', class_='regularLink nohover')\n",
    "    if table:\n",
    "        # Iterate through rows in the table\n",
    "        for row in table.find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) >= 2:  # Ensure it's a valid row\n",
    "                occupation_name = cols[0].text.strip()\n",
    "                occupation_url = cols[0].find('a')['href']\n",
    "                sub_occupations.append({'Occupation': occupation_name, 'URL': occupation_url})\n",
    "    \n",
    "    return sub_occupations\n",
    "\n",
    "# Function to scrape layer three (additional items)\n",
    "def scrape_layer_three(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Example: Scraping additional items\n",
    "    additional_items = []\n",
    "    # You need to customize this part based on the structure of the webpage\n",
    "    # Find the elements containing the additional items and extract the data\n",
    "    # For example:\n",
    "    # item1 = soup.find('div', class_='additional-item').text.strip()\n",
    "    # item2 = soup.find('div', class_='additional-item-2').text.strip()\n",
    "    # additional_items.append({'Item1': item1, 'Item2': item2})\n",
    "    \n",
    "    return additional_items\n",
    "\n",
    "# Function to scrape layer one (group pages)\n",
    "def scrape_layer_one():\n",
    "    base_url = 'https://www.bls.gov/ooh/home.htm'\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    group_pages = []\n",
    "    \n",
    "    # Find all links in the list\n",
    "    links = soup.find_all('li')\n",
    "    for link in links:\n",
    "        group_name = link.text.strip()\n",
    "        group_url = link.find('a')['href']\n",
    "        group_pages.append({'Group': group_name, 'URL': group_url})\n",
    "    \n",
    "    return group_pages\n",
    "\n",
    "# Main function to orchestrate scraping\n",
    "def main():\n",
    "    # Scrape layer one\n",
    "    group_pages = scrape_layer_one()\n",
    "    \n",
    "    # Initialize dataframe to store all data\n",
    "    df = pd.DataFrame(columns=['Group', 'Occupation', 'URL'])  # Add columns for additional items as needed\n",
    "    \n",
    "    # Iterate through group pages\n",
    "    for group in group_pages:\n",
    "        # Scrape layer two\n",
    "        sub_occupations = scrape_layer_two(group['URL'])\n",
    "        \n",
    "        # Iterate through sub-occupations\n",
    "        for sub_occ in sub_occupations:\n",
    "            # Scrape layer three\n",
    "            additional_items = scrape_layer_three(sub_occ['URL'])\n",
    "            \n",
    "            # Append data to dataframe\n",
    "            df = df.append({'Group': group['Group'], 'Occupation': sub_occ['Occupation'], 'URL': sub_occ['URL']}, ignore_index=True)\n",
    "            \n",
    "            # Append additional items to dataframe\n",
    "            # Update the dataframe with additional items as needed\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Scraped: {group['Group']} - {sub_occ['Occupation']}\")\n",
    "    \n",
    "    # Save dataframe to a CSV file\n",
    "    df.to_csv('scraped_data.csv', index=False)\n",
    "    print(\"Scraping complete. Data saved to 'scraped_data.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML>\n",
      "<html lang=\"en-us\">      \n",
      "               <head> \n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
      "<title>Access Denied</title>\n",
      "</head>\n",
      "  <style type=\"text/css\">\n",
      "    .centerDiv\n",
      "    {\n",
      "      width: 60%;\n",
      "      height:200px;\n",
      "      margin: 0 auto;\n",
      "      background-color:#FFFFFF ;\n",
      "    }\n",
      "  </style>\n",
      "<body>\n",
      "<div class=\"centerDiv\">\n",
      "               <h1><a href=https://www.bls.gov><!--img src=\"/apology_objects/images/01.jpg\" border=\"0\"--></a><span style=\"font-family: Times, serif; color: #990000; font-size: 38px;\">Bureau of Labor Statistics</span></h1>\n",
      "    <h2>Access Denied</h2>\n",
      "              \n",
      "               <p>The BLS is committed to providing data promptly and according to established schedules. Automated retrieval programs (commonly called \"robots\" or \"bots\") can cause delays and interfere with other customers' timely access to information. Therefore, bot activity that doesn't conform to BLS usage policy is prohibited.</p>\n",
      "              \n",
      " \n",
      "               <p>We apologize for any inconvenience. If you believe we have made an error, please <a href=https://data.bls.gov/forms/opb.htm?akamai-0.d9d84b17.1712307791.514d5e96>contact us</a>.</p>\n",
      "              \n",
      "               <p>Please contact your administrator with the error code: 0.d9d84b17.1712307791.514d5e96</p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "from io import StringIO\n",
    "\n",
    "base_url = 'https://www.bls.gov/ooh/home.htm'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "time.sleep(random.uniform(1, 3))  # Introduce a random delay between requests\n",
    "response = requests.get(base_url, headers=headers)\n",
    "with open('webpage.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "# Read the saved HTML file\n",
    "with open('webpage.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "    print(html_content)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve HTML content. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to save webpage HTML content to a file\n",
    "def save_html(url, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"HTML content saved to '{filename}'\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve HTML content. Status code: {response.status_code}\")\n",
    "\n",
    "# Save the HTML content of the webpage to a file\n",
    "save_html('https://www.bls.gov/ooh/home.htm', 'webpage.html')\n",
    "\n",
    "# Read the saved HTML file and create a BeautifulSoup object\n",
    "with open('webpage.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Now you can use BeautifulSoup to navigate and extract data from the HTML\n",
    "# Example:\n",
    "# title = soup.title\n",
    "# print(title.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
