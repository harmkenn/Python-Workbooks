{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 3. Classification, Decision Trees and k Nearest Neighbors\n",
    "https://mlcourse.ai/book/topic03/topic03_decision_trees_kNN.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "Before we dive into the material for this week’s article, let’s talk about the kind of problem that we are going to solve and its place in the exciting field of machine learning. T. Mitchell’s book “Machine Learning” (1997) gives a classic, general definition of machine learning as follows:\n",
    "\n",
    "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n",
    "\n",
    "In the various problem settings T, P, and E can refer to completely different things. Some of the most popular tasks T in machine learning are the following:\n",
    "\n",
    "classification of an instance to one of the categories based on its features;\n",
    "\n",
    "regression – prediction of a numerical target feature based on other features of an instance;\n",
    "\n",
    "clustering – identifying partitions of instances based on the features of these instances so that the members within the groups are more similar to each other than those in the other groups;\n",
    "\n",
    "anomaly detection – search for instances that are “greatly dissimilar” to the rest of the sample or to some group of instances;\n",
    "\n",
    "and so many more.\n",
    "\n",
    "A good overview is provided in the “Machine Learning basics” chapter of “Deep Learning” (by Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016).\n",
    "\n",
    "Experience E refers to data (we can’t go anywhere without it). Machine learning algorithms can be divided into those that are trained in supervised or unsupervised manner. In unsupervised learning tasks, one has a set consisting of instances described by a set of features. In supervised learning problems, there’s also a target variable, which is what we would like to be able to predict, known for each instance in a training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "Classification and regression are supervised learning problems. For example, as a credit institution, we may want to predict loan defaults based on the data accumulated about our clients. Here, the experience E is the available training data: a set of instances (clients), a collection of features (such as age, salary, type of loan, past loan defaults, etc.) for each, and a target variable (whether they defaulted on the loan). This target variable is just a fact of loan default (1 or 0), so recall that this is a (binary) classification problem. If you were instead predicting by how much time the loan payment is overdue, this would become a regression problem.\n",
    "\n",
    "Finally, the third term used in the definition of machine learning is a metric of the algorithm’s performance evaluation P. Such metrics differ for various problems and algorithms, and we’ll discuss them as we study new algorithms. For now, we’ll refer to a simple metric for classification algorithms, the proportion of correct answers – accuracy – on the test set.\n",
    "\n",
    "Let’s take a look at two supervised learning problems: classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Decision Tree\n",
    "We begin our overview of classification and regression methods with one of the most popular ones – a decision tree. Decision trees are used in everyday life decisions, not just in machine learning. Flow diagrams are actually visual representations of decision trees. For example, Higher School of Economics publishes information diagrams to make the lives of its employees easier. Here is a snippet of instructions for publishing a paper on the Institution portal.\n",
    "\n",
    "../../_images/topic3_hse_instruction.png\n",
    "In terms of machine learning, one can see it as a simple classifier that determines the appropriate form of publication (book, article, chapter of the book, preprint, publication in the “Higher School of Economics and the Media”) based on the content (book, pamphlet, paper), type of journal, original publication type (scientific journal, proceedings), etc.\n",
    "\n",
    "A decision tree is often a generalization of the experts’ experience, a means of sharing knowledge of a particular process. For example, before the introduction of scalable machine learning algorithms, the credit scoring task in the banking sector was solved by experts. The decision to grant a loan was made on the basis of some intuitively (or empirically) derived rules that could be represented as a decision tree.\n",
    "\n",
    "../../_images/credit_scoring_toy_tree_english.png\n",
    "In our next case, we solve a binary classification problem (approve/deny a loan) based on the “Age”, “Home-ownership”, “Income”, and “Education” features.\n",
    "\n",
    "The decision tree as a machine learning algorithm is essentially the same thing as the diagram shown above; we incorporate a stream of logical rules of the form “feature \n",
    " value is less than \n",
    " and feature \n",
    " value is less than \n",
    " … => Category 1” into a tree-like data structure. The advantage of this algorithm is that they are easily interpretable. For example, using the above scheme, the bank can explain to the client why they were denied for a loan: e.g the client does not own a house and her income is less than 5,000.\n",
    "\n",
    "As we’ll see later, many other models, although more accurate, do not have this property and can be regarded as more of a “black box” approach, where it is harder to interpret how the input data was transformed into the output. Due to this “understandability” and similarity to human decision-making (you can easily explain your model to your boss), decision trees have gained immense popularity. C4.5, a representative of this group of classification methods, is even the first in the list of 10 best data mining algorithms (“Top 10 Algorithms in Data Mining”, Knowledge and Information Systems, 2008. ResearchGate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, misclassification error is almost never used, and Gini uncertainty and information gain work similarly.\n",
    "\n",
    "For binary classification, entropy and Gini uncertainty take the following form:\n",
    "\n",
    "\n",
    "\n",
    "where (\n",
    " is the probability of an object having a label +).\n",
    "\n",
    "If we plot these two functions against the argument \n",
    ", we will see that the entropy plot is very close to the plot of Gini uncertainty, doubled. Therefore, in practice, these two criteria are almost identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't like warnings\n",
    "# you can comment the following 2 lines if you'd like to\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "xx = np.linspace(0, 1, 50)\n",
    "plt.plot(xx, [2 * x * (1 - x) for x in xx], label=\"gini\")\n",
    "plt.plot(xx, [4 * x * (1 - x) for x in xx], label=\"2*gini\")\n",
    "plt.plot(xx, [-x * np.log2(x) - (1 - x) * np.log2(1 - x) for x in xx], label=\"entropy\")\n",
    "plt.plot(xx, [1 - max(x, 1 - x) for x in xx], label=\"missclass\")\n",
    "plt.plot(xx, [2 - 2 * max(x, 1 - x) for x in xx], label=\"2*missclass\")\n",
    "plt.xlabel(\"p+\")\n",
    "plt.ylabel(\"criterion\")\n",
    "plt.title(\"Criteria of quality as a function of p+ (binary classification)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "Let’s consider fitting a decision tree to some synthetic data. We will generate samples from two classes, both normal distributions but with different means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first class\n",
    "np.random.seed(17)\n",
    "train_data = np.random.normal(size=(100, 2))\n",
    "train_labels = np.zeros(100)\n",
    "\n",
    "# adding second class\n",
    "train_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)]\n",
    "train_labels = np.r_[train_labels, np.ones(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the data. Informally, the classification problem in this case is to build some “good” boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or at least a straight line or a hyperplane, would work well on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    train_data[:, 0],\n",
    "    train_data[:, 1],\n",
    "    c=train_labels,\n",
    "    s=100,\n",
    "    cmap=\"autumn\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "plt.plot(range(-2, 5), range(4, -3, -1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try to separate these two classes by training an Sklearn decision tree. We will use max_depth parameter that limits the depth of the tree. Let’s visualize the resulting separating boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Let’s write an auxiliary function that will return grid for further visualization.\n",
    "def get_grid(data):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=17)\n",
    "\n",
    "# training the tree\n",
    "clf_tree.fit(train_data, train_labels)\n",
    "\n",
    "# some code to depict separating surface\n",
    "xx, yy = get_grid(train_data)\n",
    "predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, predicted, cmap=\"autumn\")\n",
    "plt.scatter(\n",
    "    train_data[:, 0],\n",
    "    train_data[:, 1],\n",
    "    c=train_labels,\n",
    "    s=100,\n",
    "    cmap=\"autumn\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=1.5,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how does the tree itself look? We see that the tree “cuts” the space into 8 rectangles, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus  # pip install pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "\n",
    "def tree_graph_to_png(tree, feature_names, png_file_to_save):\n",
    "    # needs graphviz to be installed\n",
    "    tree_str = export_graphviz(\n",
    "        tree, feature_names=feature_names, filled=True, out_file=None\n",
    "    )\n",
    "    graph = pydotplus.graph_from_dot_data(tree_str)\n",
    "    graph.write_png(png_file_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_graph_to_png(\n",
    "    tree=clf_tree,\n",
    "    feature_names=[\"x1\", \"x2\"],\n",
    "    png_file_to_save=\"topic3_decision_tree1.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we “read” such a tree?\n",
    "In the beginning, there were 200 samples (instances), 100 of each class. The entropy of the initial state was maximal, \n",
    ". Then, the first partition of the samples into 2 groups was made by comparing the value of \n",
    " with \n",
    " (find this part of the border in the picture above). With that, the entropy of both left and right groups decreased. The process continues up to depth 3. In this visualization, the more samples of the first class, the darker the orange color of the vertex; the more samples of the second class, the darker the blue. At the beginning, the number of samples from two classes is equal, so the root node of the tree is white.\n",
    "\n",
    "How a Decision Tree Works with Numerical Features\n",
    "Suppose we have a numeric feature “Age” that has a lot of unique values. A decision tree will look for the best (according to some criterion of information gain) split by checking binary attributes such as “Age <17”, “Age < 22.87”, and so on. But what if the age range is large? Or what if another quantitative variable, “salary”, can also be “cut” in many ways? There will be too many binary attributes to select from at each step during tree construction. To resolve this problem, heuristics are usually used to limit the number of thresholds to which we compare the quantitative variable.\n",
    "\n",
    "Let’s consider an example. Suppose we have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Loan Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Loan Default\n",
       "0    17             1\n",
       "2    18             1\n",
       "3    20             0\n",
       "7    25             1\n",
       "8    29             1\n",
       "9    31             0\n",
       "10   33             1\n",
       "4    38             1\n",
       "5    49             0\n",
       "6    55             0\n",
       "1    64             0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"Age\": [17, 64, 18, 20, 38, 49, 55, 25, 29, 31, 33],\n",
    "        \"Loan Default\": [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
    "    }\n",
    ")\n",
    "data.sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tree = DecisionTreeClassifier(random_state=17)\n",
    "age_tree.fit(data[\"Age\"].values.reshape(-1, 1), data[\"Loan Default\"].values)\n",
    "\n",
    "tree_graph_to_png(\n",
    "    age_tree,\n",
    "    feature_names=[\"Age\"],\n",
    "    png_file_to_save=\"topic3_decision_tree2.png\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
